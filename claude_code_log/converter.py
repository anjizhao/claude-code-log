#!/usr/bin/env python3
"""Convert Claude transcript JSONL files to HTML."""

import json
import re
from dataclasses import dataclass, field
from pathlib import Path
import traceback
from typing import Any, Dict, List, Optional, TYPE_CHECKING, cast

import dateparser

if TYPE_CHECKING:
    from .cache import CacheManager

from .utils import (
    format_timestamp_range,
    get_project_display_name,
    should_use_as_session_starter,
    create_session_preview,
    get_warmup_session_ids,
)
from .cache import (
    CacheManager,
    SessionCacheData,
    get_all_cached_projects,
    get_library_version,
)
from .parser import parse_timestamp
from .factories import create_transcript_entry
from .models import (
    TranscriptEntry,
    AssistantTranscriptEntry,
    CustomTitleTranscriptEntry,
    SummaryTranscriptEntry,
    SystemTranscriptEntry,
    UserTranscriptEntry,
    ToolResultContent,
)
from .renderer import get_renderer, is_html_outdated


def get_file_extension(format: str) -> str:
    """Get the file extension for a format.

    Normalizes 'markdown' to 'md' for consistent file extensions.
    """
    return "md" if format in ("md", "markdown") else format


# =============================================================================
# Transcript Loading Functions
# =============================================================================


def filter_messages_by_date(
    messages: list[TranscriptEntry], from_date: Optional[str], to_date: Optional[str]
) -> list[TranscriptEntry]:
    """Filter messages based on date range.

    Date parsing is done in UTC to match transcript timestamps which are stored in UTC.
    """
    if not from_date and not to_date:
        return messages

    # Parse dates in UTC to match transcript timestamps (which are stored in UTC)
    dateparser_settings: Any = {"TIMEZONE": "UTC", "RETURN_AS_TIMEZONE_AWARE": False}
    from_dt = None
    to_dt = None

    if from_date:
        from_dt = dateparser.parse(from_date, settings=dateparser_settings)
        if not from_dt:
            raise ValueError(f"Could not parse from-date: {from_date}")
        # If parsing relative dates like "today", start from beginning of day
        if from_date in ["today", "yesterday"] or "days ago" in from_date:
            from_dt = from_dt.replace(hour=0, minute=0, second=0, microsecond=0)

    if to_date:
        to_dt = dateparser.parse(to_date, settings=dateparser_settings)
        if not to_dt:
            raise ValueError(f"Could not parse to-date: {to_date}")
        # If parsing relative dates like "today", end at end of day
        if to_date in ["today", "yesterday"] or "days ago" in to_date:
            to_dt = to_dt.replace(hour=23, minute=59, second=59, microsecond=999999)

    filtered_messages: list[TranscriptEntry] = []
    for message in messages:
        # Handle entries without timestamp (summaries, custom titles)
        if isinstance(message, (SummaryTranscriptEntry, CustomTitleTranscriptEntry)):
            filtered_messages.append(message)
            continue

        timestamp_str = message.timestamp
        if not timestamp_str:
            continue

        message_dt = parse_timestamp(timestamp_str)
        if not message_dt:
            continue

        # Convert to naive datetime for comparison (dateparser returns naive datetimes)
        if message_dt.tzinfo:
            message_dt = message_dt.replace(tzinfo=None)

        # Check if message falls within date range
        if from_dt and message_dt < from_dt:
            continue
        if to_dt and message_dt > to_dt:
            continue

        filtered_messages.append(message)

    return filtered_messages


def load_transcript(
    jsonl_path: Path,
    cache_manager: Optional["CacheManager"] = None,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    silent: bool = False,
    _loaded_files: Optional[set[Path]] = None,
) -> list[TranscriptEntry]:
    """Load and parse JSONL transcript file, using cache if available.

    Args:
        _loaded_files: Internal parameter to track loaded files and prevent infinite recursion.
    """
    # Initialize loaded files set on first call
    if _loaded_files is None:
        _loaded_files = set()

    # Prevent infinite recursion by checking if this file is already being loaded
    if jsonl_path in _loaded_files:
        return []

    _loaded_files.add(jsonl_path)
    # Try to load from cache first
    if cache_manager is not None:
        # Use filtered loading if date parameters are provided
        if from_date or to_date:
            cached_entries = cache_manager.load_cached_entries_filtered(
                jsonl_path, from_date, to_date
            )
        else:
            cached_entries = cache_manager.load_cached_entries(jsonl_path)

        if cached_entries is not None:
            if not silent:
                print(f"Loading {jsonl_path} from cache...")
            return cached_entries

    # Parse from source file
    messages: list[TranscriptEntry] = []
    agent_ids: set[str] = set()  # Collect agentId references while parsing

    try:
        f = open(jsonl_path, "r", encoding="utf-8", errors="replace")
    except FileNotFoundError:
        # Handle race condition: file may have been deleted between glob and open
        # (e.g., Claude Code session cleanup)
        if not silent:
            print(f"Warning: File not found (may have been deleted): {jsonl_path}")
        return []

    with f:
        if not silent:
            print(f"Processing {jsonl_path}...")
        for line_no, line in enumerate(f, 1):  # Start counting from 1
            line = line.strip()
            if line:
                try:
                    entry_dict: dict[str, Any] | str = json.loads(line)
                    if not isinstance(entry_dict, dict):
                        print(
                            f"Line {line_no} of {jsonl_path} is not a JSON object: {line}"
                        )
                        continue

                    # Check for agentId BEFORE Pydantic parsing
                    # agentId can be at top level OR nested in toolUseResult
                    # For UserTranscriptEntry, we need to copy it to top level so Pydantic preserves it
                    if "agentId" in entry_dict:
                        agent_id = entry_dict.get("agentId")
                        if agent_id:
                            agent_ids.add(agent_id)
                    elif "toolUseResult" in entry_dict:
                        tool_use_result = entry_dict.get("toolUseResult")
                        if (
                            isinstance(tool_use_result, dict)
                            and "agentId" in tool_use_result
                        ):
                            agent_id_value = cast(Any, tool_use_result).get("agentId")
                            if isinstance(agent_id_value, str):
                                agent_ids.add(agent_id_value)
                                # Copy agentId to top level for Pydantic to preserve
                                entry_dict["agentId"] = agent_id_value

                    entry_type: str | None = entry_dict.get("type")

                    if entry_type in [
                        "user",
                        "assistant",
                        "summary",
                        "custom-title",
                        "system",
                        "queue-operation",
                    ]:
                        # Parse using Pydantic models
                        entry = create_transcript_entry(entry_dict)
                        messages.append(entry)
                    elif (
                        entry_type
                        in [
                            "file-history-snapshot",  # Internal Claude Code file backup metadata
                            "progress",  # Real-time progress updates (hook_progress, bash_progress)
                        ]
                    ):
                        # Silently skip internal message types we don't render
                        pass
                    else:
                        display_line = line[:1000] + "..." if len(line) > 1000 else line
                        print(
                            f"Line {line_no} of {jsonl_path} is not a recognised message type: {display_line}"
                        )
                except json.JSONDecodeError as e:
                    print(
                        f"Line {line_no} of {jsonl_path} | JSON decode error: {str(e)}"
                    )
                except ValueError as e:
                    # Extract a more descriptive error message
                    error_msg = str(e)
                    if "validation error" in error_msg.lower():
                        err_no_url = re.sub(
                            r"    For further information visit https://errors.pydantic(.*)\n?",
                            "",
                            error_msg,
                        )
                        print(f"Line {line_no} of {jsonl_path} | {err_no_url}")
                    else:
                        print(
                            f"Line {line_no} of {jsonl_path} | ValueError: {error_msg}"
                            f"\n{traceback.format_exc()}"
                        )
                except Exception as e:
                    print(
                        f"Line {line_no} of {jsonl_path} | Unexpected error: {str(e)}"
                        f"\n{traceback.format_exc()}"
                    )

    # Load agent files if any were referenced
    # Build a map of agentId -> agent messages
    agent_messages_map: dict[str, list[TranscriptEntry]] = {}
    if agent_ids:
        parent_dir = jsonl_path.parent
        session_basename = (
            jsonl_path.stem
        )  # e.g., "29ccd257-68b1-427f-ae5f-6524b7cb6f20"
        for agent_id in agent_ids:
            # Try legacy location first (same directory as session file)
            agent_file = parent_dir / f"agent-{agent_id}.jsonl"
            # Skip if the agent file is the same as the current file (self-reference)
            if agent_file == jsonl_path:
                continue
            # Try new subagents directory structure (Claude Code 2.1.2+)
            if not agent_file.exists():
                subagent_file = (
                    parent_dir
                    / session_basename
                    / "subagents"
                    / f"agent-{agent_id}.jsonl"
                )
                if subagent_file.exists():
                    agent_file = subagent_file
            if agent_file.exists():
                if not silent:
                    print(f"Loading agent file {agent_file}...")
                # Recursively load the agent file (it might reference other agents)
                agent_messages = load_transcript(
                    agent_file,
                    cache_manager,
                    from_date,
                    to_date,
                    silent=True,
                    _loaded_files=_loaded_files,
                )
                agent_messages_map[agent_id] = agent_messages

    # Insert agent messages at their point of use (only once per agent)
    if agent_messages_map:
        # Iterate through messages and insert agent messages after the FIRST message
        # that references them (via UserTranscriptEntry.agentId)
        result_messages: list[TranscriptEntry] = []
        for message in messages:
            result_messages.append(message)

            # Check if this is a UserTranscriptEntry with agentId
            if isinstance(message, UserTranscriptEntry) and message.agentId:
                agent_id = message.agentId
                if agent_id in agent_messages_map:
                    # Insert agent messages right after this message (pop to insert only once)
                    result_messages.extend(agent_messages_map.pop(agent_id))

        messages = result_messages

    # Save to cache if cache manager is available
    if cache_manager is not None:
        cache_manager.save_cached_entries(jsonl_path, messages)

    return messages


def load_directory_transcripts(
    directory_path: Path,
    cache_manager: Optional["CacheManager"] = None,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    silent: bool = False,
) -> list[TranscriptEntry]:
    """Load all JSONL transcript files from a directory and combine them."""
    all_messages: list[TranscriptEntry] = []

    # Find all .jsonl files, excluding agent files (they are loaded via load_transcript
    # when a session references them via agentId)
    jsonl_files = [
        f for f in directory_path.glob("*.jsonl") if not f.name.startswith("agent-")
    ]

    for jsonl_file in jsonl_files:
        messages = load_transcript(
            jsonl_file, cache_manager, from_date, to_date, silent
        )
        all_messages.extend(messages)

    # Sort all messages chronologically
    def get_timestamp(entry: TranscriptEntry) -> str:
        if hasattr(entry, "timestamp"):
            return entry.timestamp  # type: ignore
        return ""

    all_messages.sort(key=get_timestamp)
    return all_messages


# =============================================================================
# Deduplication
# =============================================================================


def deduplicate_messages(messages: list[TranscriptEntry]) -> list[TranscriptEntry]:
    """Remove duplicate messages based on (type, timestamp, sessionId, content_key).

    Messages with the exact same timestamp are duplicates by definition -
    the differences (like IDE selection tags) are just logging artifacts.

    We need a content-based key to handle two cases:
    1. Version stutter: Same message logged twice during Claude Code upgrade
       -> Same timestamp, same message.id or tool_use_id -> SHOULD deduplicate
    2. Concurrent tool results: Multiple tool results with same timestamp
       -> Same timestamp, different tool_use_ids -> should NOT deduplicate
    3. User text messages with same timestamp but different UUIDs (branch switch artifacts)
       -> Same timestamp, no tool_use_id -> SHOULD deduplicate, keep the one with most content

    Args:
        messages: List of transcript entries to deduplicate

    Returns:
        List of deduplicated messages, preserving order (first occurrence kept,
        but replaced in-place if a better version is found later)
    """
    # Track seen dedup_key -> index in deduplicated list (for in-place replacement)
    seen: dict[tuple[str, str, bool, str, str], int] = {}
    deduplicated: list[TranscriptEntry] = []

    for message in messages:
        # Get basic message type
        message_type = getattr(message, "type", "unknown")

        # For system messages, include level to differentiate info/warning/error
        if isinstance(message, SystemTranscriptEntry):
            level = getattr(message, "level", "info")
            message_type = f"system-{level}"

        # Get timestamp
        timestamp = getattr(message, "timestamp", "")

        # Get isMeta flag (slash command prompts have isMeta=True with same timestamp as parent)
        is_meta = getattr(message, "isMeta", False)

        # Get sessionId for multi-session report deduplication
        session_id = getattr(message, "sessionId", "")

        # Get content key for differentiating concurrent messages
        # - For assistant messages: use message.id (same for stutters, different for different msgs)
        # - For user messages with tool results: use first tool_use_id
        # - For user text messages: use empty string (deduplicate by timestamp alone)
        # - For summary messages: use leafUuid (summaries have no timestamp/uuid)
        content_key = ""
        is_user_text = False
        if isinstance(message, AssistantTranscriptEntry):
            # For assistant messages, use the message id
            content_key = message.message.id
        elif isinstance(message, UserTranscriptEntry):
            # For user messages, check for tool results
            for item in message.message.content:
                if isinstance(item, ToolResultContent):
                    content_key = item.tool_use_id
                    break
            else:
                # No tool result found - this is a user text message
                is_user_text = True
                # content_key stays empty (dedupe by timestamp alone)
        elif isinstance(message, SummaryTranscriptEntry):
            # Summaries have no timestamp or uuid - use leafUuid to keep them distinct
            content_key = message.leafUuid

        # Create deduplication key
        dedup_key = (message_type, timestamp, is_meta, session_id, content_key)

        if dedup_key in seen:
            # For user text messages, replace if new one has more content items
            if is_user_text and isinstance(message, UserTranscriptEntry):
                idx = seen[dedup_key]
                existing = deduplicated[idx]
                if isinstance(existing, UserTranscriptEntry) and len(
                    message.message.content
                ) > len(existing.message.content):
                    deduplicated[idx] = message  # Replace with better version
            # Otherwise skip duplicate
        else:
            seen[dedup_key] = len(deduplicated)
            deduplicated.append(message)

    return deduplicated


@dataclass
class GenerationStats:
    """Track statistics for HTML generation across a project."""

    # Cache statistics
    files_loaded_from_cache: int = 0
    files_updated: int = 0

    # HTML generation statistics
    sessions_total: int = 0
    sessions_regenerated: int = 0
    combined_regenerated: bool = False

    # Timing (seconds)
    cache_time: float = 0.0
    render_time: float = 0.0
    total_time: float = 0.0

    # Errors/warnings collected during processing
    warnings: List[str] = field(default_factory=lambda: [])
    errors: List[str] = field(default_factory=lambda: [])

    def add_warning(self, msg: str) -> None:
        """Add a warning message."""
        self.warnings.append(msg)

    def add_error(self, msg: str) -> None:
        """Add an error message."""
        self.errors.append(msg)

    def summary(self, project_name: str) -> str:
        """Generate a concise summary line for this project."""
        parts: List[str] = [f"Project: {project_name}"]

        # Cache info
        cache_parts: List[str] = []
        if self.files_loaded_from_cache > 0:
            cache_parts.append(f"{self.files_loaded_from_cache} cached")
        if self.files_updated > 0:
            cache_parts.append(f"{self.files_updated} updated")
        if cache_parts:
            parts.append(f"  Cache: {', '.join(cache_parts)}")

        # HTML info
        html_parts: List[str] = []
        if self.sessions_total > 0:
            html_parts.append(
                f"{self.sessions_regenerated}/{self.sessions_total} sessions"
            )
        if self.combined_regenerated:
            html_parts.append("combined")
        if html_parts:
            parts.append(f"  HTML: {', '.join(html_parts)} regenerated")
        elif self.sessions_total > 0:
            parts.append("  HTML: up to date")

        # Timing
        if self.total_time > 0:
            time_str = f"  Time: {self.total_time:.1f}s"
            if self.cache_time > 0 or self.render_time > 0:
                time_str += (
                    f" (cache: {self.cache_time:.1f}s, render: {self.render_time:.1f}s)"
                )
            parts.append(time_str)

        return "\n".join(parts)


def _get_page_html_path(page_number: int) -> str:
    """Get the HTML filename for a given page number.

    Page 1 is combined_transcripts.html, page 2+ are combined_transcripts_N.html
    """
    if page_number == 1:
        return "combined_transcripts.html"
    return f"combined_transcripts_{page_number}.html"


# Regex pattern to match and update the next link marker block
_NEXT_LINK_PATTERN = re.compile(
    r'(<!-- PAGINATION_NEXT_LINK_START -->.*?class="page-nav-link next) last-page(".*?<!-- PAGINATION_NEXT_LINK_END -->)',
    re.DOTALL,
)


def _enable_next_link_on_previous_page(output_dir: Path, page_number: int) -> bool:
    """Enable the next link on a previous page by removing the last-page class.

    When a new page is created, the previous page's "Next" link (which was hidden
    with the last-page CSS class) needs to be revealed. This function performs
    an in-place edit to remove that class.

    Args:
        output_dir: Directory containing the HTML files
        page_number: The page number whose next link should be enabled

    Returns:
        True if the file was modified, False otherwise
    """
    if page_number < 1:
        return False

    page_path = output_dir / _get_page_html_path(page_number)
    if not page_path.exists():
        return False

    content = page_path.read_text(encoding="utf-8")

    # Check if there's a last-page class to remove
    if "last-page" not in content:
        return False

    # Replace the pattern to remove last-page class
    new_content, count = _NEXT_LINK_PATTERN.subn(r"\1\2", content)

    if count > 0:
        page_path.write_text(new_content, encoding="utf-8")
        return True

    return False


def _assign_sessions_to_pages(
    sessions: Dict[str, SessionCacheData], page_size: int
) -> List[List[str]]:
    """Assign sessions to pages, never splitting sessions across pages.

    Args:
        sessions: Dict mapping session_id to SessionCacheData
        page_size: Maximum messages per page (overflow allowed to keep sessions intact)

    Returns:
        List of pages, each containing a list of session_ids
    """
    pages: List[List[str]] = []
    current_page: List[str] = []
    current_count = 0

    # Sort sessions chronologically by first_timestamp
    sorted_sessions = sorted(sessions.values(), key=lambda s: s.first_timestamp or "")

    for session in sorted_sessions:
        # Add session to current page (never split sessions)
        current_page.append(session.session_id)
        current_count += session.message_count

        # If page now exceeds limit, close it and start fresh
        if current_count > page_size:
            pages.append(current_page)
            current_page = []
            current_count = 0

    # Don't forget the last page
    if current_page:
        pages.append(current_page)

    return pages


def _build_project_summary_from_cache(
    input_path: Path,
    cache_manager: Optional["CacheManager"],
    working_directories: List[str],
) -> Dict[str, Any]:
    """Build project summary dict from cache data for the project session index."""
    project_data: Dict[str, Any] = {
        "name": input_path.name,
        "html_file": "index.html",
        "jsonl_count": len(list(input_path.glob("*.jsonl"))),
        "message_count": 0,
        "last_modified": 0.0,
        "working_directories": working_directories,
        "sessions": [],
    }

    if cache_manager is not None:
        cached = cache_manager.get_cached_project_data()
        if cached is not None:
            project_data["message_count"] = cached.total_message_count
            project_data["total_input_tokens"] = cached.total_input_tokens
            project_data["total_output_tokens"] = cached.total_output_tokens
            project_data["total_cache_creation_tokens"] = (
                cached.total_cache_creation_tokens
            )
            project_data["total_cache_read_tokens"] = cached.total_cache_read_tokens
            project_data["latest_timestamp"] = cached.latest_timestamp
            project_data["earliest_timestamp"] = cached.earliest_timestamp
            project_data["sessions"] = [
                {
                    "id": s.session_id,
                    "summary": s.custom_title or s.summary,
                    "timestamp_range": format_timestamp_range(
                        s.first_timestamp, s.last_timestamp
                    ),
                    "first_timestamp": s.first_timestamp,
                    "last_timestamp": s.last_timestamp,
                    "message_count": s.message_count,
                    "first_user_message": s.first_user_message
                    or "[No user message found in session.]",
                }
                for s in cached.sessions.values()
                if s.first_user_message and s.first_user_message != "Warmup"
            ]

    return project_data


def _build_session_data_from_messages(
    messages: List[TranscriptEntry],
) -> Dict[str, SessionCacheData]:
    """Build session data from messages when cache is unavailable.

    This is a fallback for pagination when get_cached_project_data() returns None.

    Args:
        messages: All messages (deduplicated)

    Returns:
        Dict mapping session_id to SessionCacheData
    """
    from .parser import extract_text_content

    # Pre-compute warmup session IDs to filter them out
    warmup_session_ids = get_warmup_session_ids(messages)

    # Group messages by session
    sessions: Dict[str, Dict[str, Any]] = {}
    for message in messages:
        if not hasattr(message, "sessionId") or isinstance(
            message, (SummaryTranscriptEntry, CustomTitleTranscriptEntry)
        ):
            continue

        session_id = getattr(message, "sessionId", "")
        if not session_id or session_id in warmup_session_ids:
            continue

        if session_id not in sessions:
            sessions[session_id] = {
                "first_timestamp": getattr(message, "timestamp", ""),
                "last_timestamp": getattr(message, "timestamp", ""),
                "message_count": 0,
                "first_user_message": "",
                "total_input_tokens": 0,
                "total_output_tokens": 0,
                "total_cache_creation_tokens": 0,
                "total_cache_read_tokens": 0,
            }

        sessions[session_id]["message_count"] += 1
        current_timestamp = getattr(message, "timestamp", "")
        if current_timestamp:
            sessions[session_id]["last_timestamp"] = current_timestamp

        # Get first user message for preview
        if (
            isinstance(message, UserTranscriptEntry)
            and not sessions[session_id]["first_user_message"]
            and hasattr(message, "message")
        ):
            first_user_content = extract_text_content(message.message.content)
            if should_use_as_session_starter(first_user_content):
                sessions[session_id]["first_user_message"] = create_session_preview(
                    first_user_content
                )

        # Extract token usage from assistant messages
        if isinstance(message, AssistantTranscriptEntry) and hasattr(
            message, "message"
        ):
            msg_data = message.message
            if hasattr(msg_data, "usage") and msg_data.usage:
                usage = msg_data.usage
                sessions[session_id]["total_input_tokens"] += (
                    getattr(usage, "input_tokens", 0) or 0
                )
                sessions[session_id]["total_output_tokens"] += (
                    getattr(usage, "output_tokens", 0) or 0
                )
                sessions[session_id]["total_cache_creation_tokens"] += (
                    getattr(usage, "cache_creation_input_tokens", 0) or 0
                )
                sessions[session_id]["total_cache_read_tokens"] += (
                    getattr(usage, "cache_read_input_tokens", 0) or 0
                )

    # Convert to Dict[str, SessionCacheData]
    result: Dict[str, SessionCacheData] = {}
    for session_id, data in sessions.items():
        result[session_id] = SessionCacheData(
            session_id=session_id,
            first_timestamp=data["first_timestamp"],
            last_timestamp=data["last_timestamp"],
            message_count=data["message_count"],
            first_user_message=data["first_user_message"],
            total_input_tokens=data["total_input_tokens"],
            total_output_tokens=data["total_output_tokens"],
            total_cache_creation_tokens=data["total_cache_creation_tokens"],
            total_cache_read_tokens=data["total_cache_read_tokens"],
        )

    return result


def _generate_paginated_html(
    messages: List[TranscriptEntry],
    output_dir: Path,
    title: str,
    page_size: int,
    cache_manager: "CacheManager",
    session_data: Dict[str, SessionCacheData],
    working_directories: List[str],
    silent: bool = False,
    show_stats: bool = False,
) -> Path:
    """Generate paginated HTML files for combined transcript.

    Args:
        messages: All messages (deduplicated)
        output_dir: Directory to write HTML files
        title: Base title for the pages
        page_size: Maximum messages per page
        cache_manager: Cache manager for the project
        session_data: Session metadata from cache
        working_directories: Working directories for project display name
        silent: Suppress verbose output

    Returns:
        Path to the first page (combined_transcripts.html)
    """
    from .html.renderer import generate_html
    from .utils import format_timestamp

    # Check if page size changed - if so, invalidate all pages
    cached_page_size = cache_manager.get_page_size_config()
    if cached_page_size is not None and cached_page_size != page_size:
        if not silent:
            print(
                f"Page size changed from {cached_page_size} to {page_size}, regenerating all pages"
            )
        old_paths = cache_manager.invalidate_all_pages()
        # Delete old page files
        for html_path in old_paths:
            page_file = output_dir / html_path
            if page_file.exists():
                page_file.unlink()

    # Assign sessions to pages
    pages: List[List[str]] = _assign_sessions_to_pages(session_data, page_size)

    if not pages:
        # No sessions, generate empty page
        pages = [[]]

    # Clean up orphan pages if page count decreased
    old_page_count = cache_manager.get_page_count()
    new_page_count = len(pages)
    if old_page_count > new_page_count:
        for orphan_page_num in range(new_page_count + 1, old_page_count + 1):
            orphan_path = output_dir / _get_page_html_path(orphan_page_num)
            if orphan_path.exists():
                orphan_path.unlink()

    # Group messages by session for fast lookup
    messages_by_session: Dict[str, List[TranscriptEntry]] = {}
    for msg in messages:
        session_id = getattr(msg, "sessionId", None)
        if session_id:
            if session_id not in messages_by_session:
                messages_by_session[session_id] = []
            messages_by_session[session_id].append(msg)

    first_page_path = output_dir / _get_page_html_path(1)

    # Generate each page
    for page_num, page_session_ids in enumerate(pages, start=1):
        html_path = _get_page_html_path(page_num)
        page_file = output_dir / html_path

        # Check if page is stale
        is_stale, reason = cache_manager.is_page_stale(
            page_num, page_size, show_stats=show_stats
        )

        if not is_stale and page_file.exists():
            if not silent:
                print(f"Page {page_num} is current, skipping regeneration")
            continue

        if not silent:
            print(f"Generating page {page_num} ({reason})...")

        # Collect messages for this page
        page_messages: List[TranscriptEntry] = []
        for session_id in page_session_ids:
            if session_id in messages_by_session:
                page_messages.extend(messages_by_session[session_id])

        # Calculate page stats
        page_message_count = len(page_messages)
        first_timestamp = None
        last_timestamp = None
        total_input_tokens = 0
        total_output_tokens = 0
        total_cache_creation_tokens = 0
        total_cache_read_tokens = 0

        for session_id in page_session_ids:
            if session_id in session_data:
                s = session_data[session_id]
                if s.first_timestamp and (
                    first_timestamp is None or s.first_timestamp < first_timestamp
                ):
                    first_timestamp = s.first_timestamp
                if s.last_timestamp and (
                    last_timestamp is None or s.last_timestamp > last_timestamp
                ):
                    last_timestamp = s.last_timestamp
                total_input_tokens += s.total_input_tokens
                total_output_tokens += s.total_output_tokens
                total_cache_creation_tokens += s.total_cache_creation_tokens
                total_cache_read_tokens += s.total_cache_read_tokens

        # Build page_info for navigation
        has_prev = page_num > 1
        is_last_page = page_num == len(pages)

        page_info = {
            "page_number": page_num,
            "prev_link": _get_page_html_path(page_num - 1) if has_prev else None,
            "next_link": _get_page_html_path(page_num + 1),  # Always provide
            "is_last_page": is_last_page,
        }

        # Enable previous page's next link when creating a new page
        if page_num > 1:
            _enable_next_link_on_previous_page(output_dir, page_num - 1)

        # Build page_stats
        date_range = ""
        if first_timestamp and last_timestamp:
            first_fmt = format_timestamp(first_timestamp)
            last_fmt = format_timestamp(last_timestamp)
            if first_fmt == last_fmt:
                date_range = first_fmt
            else:
                date_range = f"{first_fmt} - {last_fmt}"
        elif first_timestamp:
            date_range = format_timestamp(first_timestamp)

        token_parts: List[str] = []
        if total_input_tokens:
            token_parts.append(f"Input: {total_input_tokens:,}")
        if total_output_tokens:
            token_parts.append(f"Output: {total_output_tokens:,}")
        if total_cache_creation_tokens:
            token_parts.append(f"Cache Create: {total_cache_creation_tokens:,}")
        if total_cache_read_tokens:
            token_parts.append(f"Cache Read: {total_cache_read_tokens:,}")
        token_summary = " | ".join(token_parts) if token_parts else None

        page_stats = {
            "message_count": page_message_count,
            "date_range": date_range,
            "token_summary": token_summary,
        }

        # Generate HTML for this page
        page_title = f"{title} - Page {page_num}" if page_num > 1 else title
        html_content = generate_html(
            page_messages,
            page_title,
            page_info=page_info,
            page_stats=page_stats,
            show_stats=show_stats,
        )
        page_file.write_text(html_content, encoding="utf-8")

        # Update cache
        cache_manager.update_page_cache(
            page_number=page_num,
            html_path=html_path,
            page_size_config=page_size,
            session_ids=page_session_ids,
            message_count=page_message_count,
            first_timestamp=first_timestamp,
            last_timestamp=last_timestamp,
            total_input_tokens=total_input_tokens,
            total_output_tokens=total_output_tokens,
            total_cache_creation_tokens=total_cache_creation_tokens,
            total_cache_read_tokens=total_cache_read_tokens,
            show_stats=show_stats,
        )

    return first_page_path


def convert_jsonl_to_html(
    input_path: Path,
    output_path: Optional[Path] = None,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    generate_individual_sessions: bool = True,
    use_cache: bool = True,
    silent: bool = False,
    page_size: int = 2000,
) -> Path:
    """Convert JSONL transcript(s) to HTML file(s).

    Convenience wrapper around convert_jsonl_to() for HTML format.
    """
    return convert_jsonl_to(
        "html",
        input_path,
        output_path,
        from_date,
        to_date,
        generate_individual_sessions,
        use_cache,
        silent,
        page_size=page_size,
    )


def convert_jsonl_to(
    format: str,
    input_path: Path,
    output_path: Optional[Path] = None,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    generate_individual_sessions: bool = True,
    use_cache: bool = True,
    silent: bool = False,
    image_export_mode: Optional[str] = None,
    page_size: int = 2000,
    skip_combined: bool = False,
    show_stats: bool = False,
) -> Path:
    """Convert JSONL transcript(s) to the specified format.

    Args:
        format: Output format ("html", "md", or "markdown").
        input_path: Path to JSONL file or directory.
        output_path: Optional output path.
        from_date: Optional start date filter.
        to_date: Optional end date filter.
        generate_individual_sessions: Whether to generate individual session files.
        use_cache: Whether to use caching.
        silent: Whether to suppress output.
        image_export_mode: Image export mode ("placeholder", "embedded", "referenced").
        page_size: Maximum messages per page for combined transcript pagination.
            If None, uses format default (embedded for HTML, referenced for Markdown).
        skip_combined: Skip combined transcript, generate project session index instead.
    """
    if not input_path.exists():
        raise FileNotFoundError(f"Input path not found: {input_path}")

    # Initialize cache manager for directory mode
    cache_manager = None
    if use_cache and input_path.is_dir():
        try:
            library_version = get_library_version()
            cache_manager = CacheManager(input_path, library_version)
        except Exception as e:
            print(f"Warning: Failed to initialize cache manager: {e}")

    ext = get_file_extension(format)

    # Initialize working_directories for both branches (used by pagination in directory mode)
    working_directories: List[str] = []

    if input_path.is_file():
        # Single file mode - cache only available for directory mode
        if output_path is None:
            output_path = input_path.with_suffix(f".{ext}")
        messages = load_transcript(input_path, silent=silent)
        title = f"Claude Transcript - {input_path.stem}"
        cache_was_updated = False  # No cache in single file mode
    else:
        # Directory mode - Cache-First Approach
        if output_path is None:
            if skip_combined:
                output_path = input_path / f"index.{ext}"
            else:
                output_path = input_path / f"combined_transcripts.{ext}"

        # Phase 1: Ensure cache is fresh and populated
        cache_was_updated = ensure_fresh_cache(
            input_path, cache_manager, from_date, to_date, silent
        )

        # Phase 1b: Early exit if nothing needs regeneration
        # Skip expensive message loading if all HTML is up to date
        if (
            cache_manager is not None
            and not cache_was_updated
            and from_date is None
            and to_date is None
        ):
            if skip_combined:
                # When skipping combined, check project index + session staleness
                # Use cache staleness check (tracks show_stats/skip_combined flags)
                # in addition to file-level version check
                index_cache_stale, _ = cache_manager.is_html_stale(
                    output_path.name, None, show_stats, skip_combined
                )
                project_index_stale = (
                    index_cache_stale
                    or is_html_outdated(output_path)
                    or not output_path.exists()
                )
                stale_sessions = cache_manager.get_stale_sessions(
                    show_stats=show_stats, skip_combined=skip_combined
                )
                if not project_index_stale and (
                    not stale_sessions or not generate_individual_sessions
                ):
                    if not silent:
                        print(
                            f"All HTML files are current for {input_path.name}, "
                            "skipping regeneration"
                        )
                    return output_path
            else:
                # Check if combined HTML is stale
                combined_stale, _ = cache_manager.is_html_stale(
                    output_path.name, None, show_stats, skip_combined
                )
                if not combined_stale and not is_html_outdated(output_path):
                    # Check if any session HTML is stale
                    stale_sessions = cache_manager.get_stale_sessions(
                        show_stats=show_stats, skip_combined=skip_combined
                    )
                    if not stale_sessions or not generate_individual_sessions:
                        # Nothing needs regeneration - skip loading
                        if not silent:
                            print(
                                f"All HTML files are current for {input_path.name}, "
                                "skipping regeneration"
                            )
                        return output_path

        # Phase 2: Load messages (will use fresh cache when available)
        messages = load_directory_transcripts(
            input_path, cache_manager, from_date, to_date, silent
        )

        # Get working directories from cache
        working_directories = (
            cache_manager.get_working_directories() if cache_manager else []
        )

        project_title = get_project_display_name(input_path.name, working_directories)
        title = f"Claude Transcripts - {project_title}"

    # Apply date filtering
    messages = filter_messages_by_date(messages, from_date, to_date)

    # Deduplicate messages (removes version stutters while preserving concurrent tool results)
    messages = deduplicate_messages(messages)

    # Update title to include date range if specified
    if from_date or to_date:
        date_range_parts: list[str] = []
        if from_date:
            date_range_parts.append(f"from {from_date}")
        if to_date:
            date_range_parts.append(f"to {to_date}")
        date_range_str = " ".join(date_range_parts)
        title += f" ({date_range_str})"

    # Generate output file (check if regeneration needed)
    assert output_path is not None
    renderer = get_renderer(format, image_export_mode)
    cached_data = cache_manager.get_cached_project_data() if cache_manager else None
    total_message_count = (
        cached_data.total_message_count if cached_data else len(messages)
    )

    if skip_combined and input_path.is_dir():
        # Generate lightweight project session index instead of combined transcript
        if cache_manager is not None:
            is_stale, _reason = cache_manager.is_html_stale(
                output_path.name,
                None,
                show_stats=show_stats,
                skip_combined=skip_combined,
            )
            should_regenerate = (
                is_stale
                or renderer.is_outdated(output_path)
                or not output_path.exists()
                or cache_was_updated
            )
        else:
            # Fallback without cache: can't track render flag changes
            should_regenerate = (
                renderer.is_outdated(output_path)
                or not output_path.exists()
                or cache_was_updated
            )
        if should_regenerate:
            project_data = _build_project_summary_from_cache(
                input_path, cache_manager, working_directories
            )
            from .html.renderer import generate_project_sessions_index_html

            content = generate_project_sessions_index_html(
                project_data, show_stats=show_stats
            )
            output_path.write_text(content, encoding="utf-8")

            # Update html_cache for project index (HTML only)
            if format == "html" and cache_manager is not None:
                cache_manager.update_html_cache(
                    output_path.name,
                    None,
                    total_message_count,
                    show_stats=show_stats,
                    skip_combined=skip_combined,
                )
        elif not silent:
            print(f"Project index {output_path.name} is current, skipping regeneration")
    else:
        # Generate combined transcript (paginated or single-file)
        existing_page_count = cache_manager.get_page_count() if cache_manager else 0

        # Decide whether to use pagination (HTML only, directory mode, no date filter)
        use_pagination = False
        if (
            format == "html"
            and cache_manager is not None
            and input_path.is_dir()
            and from_date is None
            and to_date is None
        ):
            use_pagination = total_message_count > page_size or existing_page_count > 1

        if use_pagination:
            # Use paginated HTML generation
            assert cache_manager is not None  # Ensured by use_pagination condition
            # Use cached session data if available, otherwise build from messages
            if cached_data is not None:
                warmup_session_ids = get_warmup_session_ids(messages)
                current_session_ids: set[str] = set()
                for message in messages:
                    session_id = getattr(message, "sessionId", "")
                    if session_id and session_id not in warmup_session_ids:
                        current_session_ids.add(session_id)
                session_data = {
                    session_id: session_cache
                    for session_id, session_cache in cached_data.sessions.items()
                    if session_id in current_session_ids
                }
            else:
                session_data = _build_session_data_from_messages(messages)
            output_path = _generate_paginated_html(
                messages,
                input_path,
                title,
                page_size,
                cache_manager,
                session_data,
                working_directories,
                silent=silent,
                show_stats=show_stats,
            )
        else:
            # Use single-file generation for small projects or filtered views
            # Use incremental regeneration via html_cache when available
            if cache_manager is not None and input_path.is_dir():
                is_stale, _reason = cache_manager.is_html_stale(
                    output_path.name,
                    None,
                    show_stats=show_stats,
                )
                should_regenerate = (
                    is_stale
                    or renderer.is_outdated(output_path)
                    or from_date is not None
                    or to_date is not None
                    or not output_path.exists()
                )
            else:
                # Fallback: old logic for single file mode or no cache
                # Note: without cache, render flag changes (show_stats, skip_combined)
                # cannot be tracked. Use cache (default) for reliable flag tracking.
                should_regenerate = (
                    renderer.is_outdated(output_path)
                    or from_date is not None
                    or to_date is not None
                    or not output_path.exists()
                    or (input_path.is_dir() and cache_was_updated)
                )

            if should_regenerate:
                # For referenced images, pass the output directory
                output_dir = output_path.parent
                content = renderer.generate(
                    messages, title, output_dir=output_dir, show_stats=show_stats
                )
                assert content is not None
                output_path.write_text(content, encoding="utf-8")

                # Update html_cache for combined transcript (HTML only)
                if format == "html" and cache_manager is not None:
                    cache_manager.update_html_cache(
                        output_path.name,
                        None,
                        total_message_count,
                        show_stats=show_stats,
                    )
            elif not silent:
                print(
                    f"{format.upper()} file {output_path.name} is current, skipping regeneration"
                )

    # Generate individual session files if requested and in directory mode
    if generate_individual_sessions and input_path.is_dir():
        _generate_individual_session_files(
            format,
            messages,
            input_path,
            from_date,
            to_date,
            cache_manager,
            cache_was_updated,
            image_export_mode,
            silent=silent,
            skip_combined=skip_combined,
            show_stats=show_stats,
        )

    return output_path


def has_cache_changes(
    project_dir: Path,
    cache_manager: Optional[CacheManager],
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
) -> bool:
    """Check if cache needs updating (fast mtime comparison only).

    Returns True if there are modified files or cache is stale.
    Does NOT load any messages - that's deferred to ensure_fresh_cache.
    """
    if cache_manager is None:
        return True  # No cache means we need to process

    jsonl_files = list(project_dir.glob("*.jsonl"))
    if not jsonl_files:
        return False

    # Get cached project data
    cached_project_data = cache_manager.get_cached_project_data()

    # Check various invalidation conditions
    modified_files = cache_manager.get_modified_files(jsonl_files)

    return (
        cached_project_data is None
        or from_date is not None
        or to_date is not None
        or bool(modified_files)
        or (cached_project_data.total_message_count == 0 and bool(jsonl_files))
    )


def ensure_fresh_cache(
    project_dir: Path,
    cache_manager: Optional[CacheManager],
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    silent: bool = False,
) -> bool:
    """Ensure cache is fresh and populated. Returns True if cache was updated.

    This does the heavy lifting of loading and parsing files.
    Call has_cache_changes() first for a fast check.
    """
    if cache_manager is None:
        return False

    # Check if cache needs updating
    # Exclude agent files from direct check - they are loaded via session references
    # Note: If only an agent file changes (session unchanged), cache won't detect it.
    # This is acceptable since agent files typically change alongside their sessions.
    session_jsonl_files = [
        f for f in project_dir.glob("*.jsonl") if not f.name.startswith("agent-")
    ]
    if not session_jsonl_files:
        return False

    # Get cached project data
    cached_project_data = cache_manager.get_cached_project_data()

    # Check various invalidation conditions
    modified_files = cache_manager.get_modified_files(session_jsonl_files)
    needs_update = (
        cached_project_data is None
        or from_date is not None
        or to_date is not None
        or bool(modified_files)  # Session files changed
        or (
            cached_project_data.total_message_count == 0 and session_jsonl_files
        )  # Stale cache
    )

    if not needs_update:
        return False  # Cache is already fresh

    # Load and process messages to populate cache
    if not silent:
        print(f"Updating cache for {project_dir.name}...")
    messages = load_directory_transcripts(
        project_dir, cache_manager, from_date, to_date, silent
    )

    # Update cache with fresh data
    _update_cache_with_session_data(cache_manager, messages)
    return True


def _update_cache_with_session_data(
    cache_manager: CacheManager, messages: list[TranscriptEntry]
) -> None:
    """Update cache with session and project aggregate data."""
    from .parser import extract_text_content

    # Collect session data (similar to _collect_project_sessions but for cache)
    session_summaries: dict[str, str] = {}
    uuid_to_session: dict[str, str] = {}
    uuid_to_session_backup: dict[str, str] = {}

    # Build mapping from message UUID to session ID
    for message in messages:
        if hasattr(message, "uuid") and hasattr(message, "sessionId"):
            message_uuid = getattr(message, "uuid", "")
            session_id = getattr(message, "sessionId", "")
            if message_uuid and session_id:
                if type(message) is AssistantTranscriptEntry:
                    uuid_to_session[message_uuid] = session_id
                else:
                    uuid_to_session_backup[message_uuid] = session_id

    # Map summaries to sessions
    for message in messages:
        if isinstance(message, SummaryTranscriptEntry):
            leaf_uuid = message.leafUuid
            if leaf_uuid in uuid_to_session:
                session_summaries[uuid_to_session[leaf_uuid]] = message.summary
            elif (
                leaf_uuid in uuid_to_session_backup
                and uuid_to_session_backup[leaf_uuid] not in session_summaries
            ):
                session_summaries[uuid_to_session_backup[leaf_uuid]] = message.summary

    # Collect custom titles (from /rename command)
    custom_titles: dict[str, str] = {}
    for message in messages:
        if isinstance(message, CustomTitleTranscriptEntry):
            custom_titles[message.sessionId] = message.customTitle

    # Group messages by session and calculate session data
    sessions_cache_data: dict[str, SessionCacheData] = {}

    # Track token usage and timestamps for project aggregates
    total_input_tokens = 0
    total_output_tokens = 0
    total_cache_creation_tokens = 0
    total_cache_read_tokens = 0
    total_message_count = len(messages)
    earliest_timestamp = ""
    latest_timestamp = ""
    seen_request_ids: set[str] = set()

    for message in messages:
        # Update project-level timestamp tracking
        if hasattr(message, "timestamp"):
            message_timestamp = getattr(message, "timestamp", "")
            if message_timestamp:
                if not latest_timestamp or message_timestamp > latest_timestamp:
                    latest_timestamp = message_timestamp
                if not earliest_timestamp or message_timestamp < earliest_timestamp:
                    earliest_timestamp = message_timestamp

        # Process session-level data (skip summaries and custom titles)
        if hasattr(message, "sessionId") and not isinstance(
            message, (SummaryTranscriptEntry, CustomTitleTranscriptEntry)
        ):
            session_id = getattr(message, "sessionId", "")
            if not session_id:
                continue

            if session_id not in sessions_cache_data:
                sessions_cache_data[session_id] = SessionCacheData(
                    session_id=session_id,
                    custom_title=custom_titles.get(session_id),
                    summary=session_summaries.get(session_id),
                    first_timestamp=getattr(message, "timestamp", ""),
                    last_timestamp=getattr(message, "timestamp", ""),
                    message_count=0,
                    first_user_message="",
                    cwd=getattr(message, "cwd", None),
                )

            session_cache = sessions_cache_data[session_id]
            session_cache.message_count += 1
            current_timestamp = getattr(message, "timestamp", "")
            if current_timestamp:
                session_cache.last_timestamp = current_timestamp

            # Get first user message for preview
            if (
                isinstance(message, UserTranscriptEntry)
                and not session_cache.first_user_message
                and hasattr(message, "message")
            ):
                first_user_content = extract_text_content(message.message.content)
                if should_use_as_session_starter(first_user_content):
                    session_cache.first_user_message = create_session_preview(
                        first_user_content
                    )

        # Calculate token usage for assistant messages
        if message.type == "assistant" and hasattr(message, "message"):
            assistant_message = getattr(message, "message")
            request_id = getattr(message, "requestId", None)
            session_id = getattr(message, "sessionId", "")

            if (
                hasattr(assistant_message, "usage")
                and assistant_message.usage
                and request_id
                and request_id not in seen_request_ids
            ):
                seen_request_ids.add(request_id)
                usage = assistant_message.usage

                # Add to project totals
                total_input_tokens += usage.input_tokens or 0
                total_output_tokens += usage.output_tokens or 0
                if usage.cache_creation_input_tokens:
                    total_cache_creation_tokens += usage.cache_creation_input_tokens
                if usage.cache_read_input_tokens:
                    total_cache_read_tokens += usage.cache_read_input_tokens

                # Add to session totals
                if session_id in sessions_cache_data:
                    session_cache = sessions_cache_data[session_id]
                    session_cache.total_input_tokens += usage.input_tokens or 0
                    session_cache.total_output_tokens += usage.output_tokens or 0
                    if usage.cache_creation_input_tokens:
                        session_cache.total_cache_creation_tokens += (
                            usage.cache_creation_input_tokens
                        )
                    if usage.cache_read_input_tokens:
                        session_cache.total_cache_read_tokens += (
                            usage.cache_read_input_tokens
                        )

    # Filter out warmup-only and empty sessions before caching
    warmup_session_ids = get_warmup_session_ids(messages)
    sessions_cache_data = {
        sid: data
        for sid, data in sessions_cache_data.items()
        if sid not in warmup_session_ids
        and data.first_user_message  # Filter empty sessions (agent-only)
    }

    # Update cache with filtered session data
    cache_manager.update_session_cache(sessions_cache_data)

    # Update cache with project aggregates
    cache_manager.update_project_aggregates(
        total_message_count=total_message_count,
        total_input_tokens=total_input_tokens,
        total_output_tokens=total_output_tokens,
        total_cache_creation_tokens=total_cache_creation_tokens,
        total_cache_read_tokens=total_cache_read_tokens,
        earliest_timestamp=earliest_timestamp,
        latest_timestamp=latest_timestamp,
    )


def _collect_project_sessions(messages: list[TranscriptEntry]) -> list[dict[str, Any]]:
    """Collect session data for project index navigation."""
    from .parser import extract_text_content

    # Pre-compute warmup session IDs to filter them out
    warmup_session_ids = get_warmup_session_ids(messages)

    # Pre-process to find and attach session summaries
    # This matches the logic from renderer.py generate_html() exactly
    session_summaries: dict[str, str] = {}
    uuid_to_session: dict[str, str] = {}
    uuid_to_session_backup: dict[str, str] = {}

    # Build mapping from message UUID to session ID across ALL messages
    # This allows summaries from later sessions to be matched to earlier sessions
    for message in messages:
        if hasattr(message, "uuid") and hasattr(message, "sessionId"):
            message_uuid = getattr(message, "uuid", "")
            session_id = getattr(message, "sessionId", "")
            if message_uuid and session_id:
                # There is often duplication, in that case we want to prioritise the assistant
                # message because summaries are generated from Claude's (last) success message
                if type(message) is AssistantTranscriptEntry:
                    uuid_to_session[message_uuid] = session_id
                else:
                    uuid_to_session_backup[message_uuid] = session_id

    # Map summaries to sessions via leafUuid -> message UUID -> session ID
    # Summaries can be in different sessions than the messages they summarize
    for message in messages:
        if isinstance(message, SummaryTranscriptEntry):
            leaf_uuid = message.leafUuid
            if leaf_uuid in uuid_to_session:
                session_summaries[uuid_to_session[leaf_uuid]] = message.summary
            elif (
                leaf_uuid in uuid_to_session_backup
                and uuid_to_session_backup[leaf_uuid] not in session_summaries
            ):
                session_summaries[uuid_to_session_backup[leaf_uuid]] = message.summary

    # Collect custom titles (from /rename command)
    custom_titles: dict[str, str] = {}
    for message in messages:
        if isinstance(message, CustomTitleTranscriptEntry):
            custom_titles[message.sessionId] = message.customTitle

    # Group messages by session (excluding warmup-only sessions)
    sessions: dict[str, dict[str, Any]] = {}
    for message in messages:
        if hasattr(message, "sessionId") and not isinstance(
            message, (SummaryTranscriptEntry, CustomTitleTranscriptEntry)
        ):
            session_id = getattr(message, "sessionId", "")
            if not session_id or session_id in warmup_session_ids:
                continue

            if session_id not in sessions:
                sessions[session_id] = {
                    "id": session_id,
                    "summary": custom_titles.get(session_id)
                    or session_summaries.get(session_id),
                    "first_timestamp": getattr(message, "timestamp", ""),
                    "last_timestamp": getattr(message, "timestamp", ""),
                    "message_count": 0,
                    "first_user_message": "",
                }

            sessions[session_id]["message_count"] += 1
            current_timestamp = getattr(message, "timestamp", "")
            if current_timestamp:
                sessions[session_id]["last_timestamp"] = current_timestamp

            # Get first user message for preview (skip system messages)
            if (
                isinstance(message, UserTranscriptEntry)
                and not sessions[session_id]["first_user_message"]
                and hasattr(message, "message")
            ):
                first_user_content = extract_text_content(message.message.content)
                if should_use_as_session_starter(first_user_content):
                    sessions[session_id]["first_user_message"] = create_session_preview(
                        first_user_content
                    )

    # Convert to list format with formatted timestamps
    session_list: list[dict[str, Any]] = []
    for session_data in sessions.values():
        timestamp_range = format_timestamp_range(
            session_data["first_timestamp"],
            session_data["last_timestamp"],
        )
        session_dict: dict[str, Any] = {
            "id": session_data["id"],
            "summary": session_data["summary"],
            "timestamp_range": timestamp_range,
            "message_count": session_data["message_count"],
            "first_user_message": session_data["first_user_message"]
            if session_data["first_user_message"] != ""
            else "[No user message found in session.]",
        }
        # Skip sessions with no user messages (empty sessions / agent-only)
        if session_data["first_user_message"] == "":
            continue
        session_list.append(session_dict)

    # Sort by first timestamp (ascending order, oldest first like transcript page)
    return sorted(
        session_list, key=lambda s: s.get("timestamp_range", ""), reverse=False
    )


def _generate_individual_session_files(
    format: str,
    messages: list[TranscriptEntry],
    output_dir: Path,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    cache_manager: Optional["CacheManager"] = None,
    cache_was_updated: bool = False,
    image_export_mode: Optional[str] = None,
    silent: bool = False,
    skip_combined: bool = False,
    show_stats: bool = False,
) -> int:
    """Generate individual files for each session in the specified format.

    Returns:
        Number of sessions regenerated
    """
    ext = get_file_extension(format)
    # Pre-compute warmup sessions to exclude them
    warmup_session_ids = get_warmup_session_ids(messages)

    # Find all unique session IDs (excluding warmup sessions)
    session_ids: set[str] = set()
    for message in messages:
        if hasattr(message, "sessionId"):
            session_id: str = getattr(message, "sessionId")
            if session_id and session_id not in warmup_session_ids:
                session_ids.add(session_id)

    # Get session data from cache for better titles
    session_data: dict[str, Any] = {}
    working_directories: list[str] = []
    if cache_manager is not None:
        project_cache = cache_manager.get_cached_project_data()
        if project_cache:
            session_data = {s.session_id: s for s in project_cache.sessions.values()}
        # Get working directories for project title
        working_directories = cache_manager.get_working_directories()

    # Only generate HTML for sessions that are tracked in the sessions table
    # (filters out warmup-only and sessions without user messages)
    session_ids = session_ids & set(session_data.keys())

    project_title = get_project_display_name(output_dir.name, working_directories)

    # Get renderer once outside the loop
    renderer = get_renderer(format, image_export_mode)
    regenerated_count = 0

    # Generate HTML file for each session
    for session_id in session_ids:
        # Create session-specific title using cache data if available
        if session_id in session_data:
            session_cache = session_data[session_id]
            if session_cache.custom_title:
                session_title = f"{project_title}: {session_cache.custom_title}"
            elif session_cache.summary:
                session_title = f"{project_title}: {session_cache.summary}"
            else:
                # Fall back to first user message preview
                preview = session_cache.first_user_message
                if preview and len(preview) > 50:
                    preview = preview[:50] + "..."
                session_title = (
                    f"{project_title}: {preview}"
                    if preview
                    else f"{project_title}: Session {session_id[:8]}"
                )
        else:
            # Fall back to basic session title
            session_title = f"{project_title}: Session {session_id[:8]}"

        # Add date range if specified
        if from_date or to_date:
            date_range_parts: list[str] = []
            if from_date:
                date_range_parts.append(f"from {from_date}")
            if to_date:
                date_range_parts.append(f"to {to_date}")
            date_range_str = " ".join(date_range_parts)
            session_title += f" ({date_range_str})"

        # Check if session file needs regeneration
        session_file_path = output_dir / f"session-{session_id}.{ext}"
        session_file_name = f"session-{session_id}.{ext}"

        # Use incremental regeneration: check per-session staleness via html_cache
        if cache_manager is not None and format == "html":
            is_stale, _reason = cache_manager.is_html_stale(
                session_file_name,
                session_id,
                show_stats=show_stats,
                skip_combined=skip_combined,
            )
            should_regenerate_session = (
                is_stale
                or renderer.is_outdated(session_file_path)
                or from_date is not None
                or to_date is not None
                or not session_file_path.exists()
            )
        else:
            # Fallback without cache or non-HTML formats
            # Note: without cache, render flag changes (show_stats, skip_combined)
            # cannot be tracked. Use cache (default) for reliable flag tracking.
            should_regenerate_session = (
                renderer.is_outdated(session_file_path)
                or from_date is not None
                or to_date is not None
                or not session_file_path.exists()
                or cache_was_updated
            )

        if should_regenerate_session:
            # Generate session content
            session_content = renderer.generate_session(
                messages,
                session_id,
                session_title,
                cache_manager,
                output_dir,
                skip_combined=skip_combined,
                show_stats=show_stats,
            )
            assert session_content is not None
            # Write session file
            session_file_path.write_text(session_content, encoding="utf-8")
            regenerated_count += 1

            # Update html_cache to track this generation (HTML only)
            if cache_manager is not None and format == "html":
                # Use message count from cache (pre-deduplication) to match
                # the count used in is_html_stale()
                if session_id in session_data:
                    session_message_count = session_data[session_id].message_count
                else:
                    # Fallback: count from messages list (less accurate due to dedup)
                    session_message_count = sum(
                        1
                        for m in messages
                        if hasattr(m, "sessionId")
                        and getattr(m, "sessionId") == session_id
                    )
                cache_manager.update_html_cache(
                    session_file_name,
                    session_id,
                    session_message_count,
                    show_stats=show_stats,
                    skip_combined=skip_combined,
                )
        elif not silent:
            print(
                f"Session file {session_file_path.name} is current, skipping regeneration"
            )

    return regenerated_count


def _get_cleanup_period_days() -> Optional[int]:
    """Read cleanupPeriodDays from Claude Code settings.

    Checks ~/.claude/settings.json for the cleanupPeriodDays setting.

    Returns:
        The configured cleanup period in days, or None if not set/readable.
    """
    import json

    settings_path = Path.home() / ".claude" / "settings.json"
    if not settings_path.exists():
        return None

    try:
        with open(settings_path, "r", encoding="utf-8") as f:
            settings = json.load(f)
        return settings.get("cleanupPeriodDays")
    except (json.JSONDecodeError, OSError):
        return None


def _print_archived_sessions_note(total_archived: int) -> None:
    """Print a note about archived sessions and how to restore them.

    Args:
        total_archived: Total number of archived sessions across all projects.
    """
    cleanup_days = _get_cleanup_period_days()
    cleanup_info = (
        f" (cleanupPeriodDays: {cleanup_days})"
        if cleanup_days is not None
        else " (cleanupPeriodDays: 30 default)"
    )

    print(
        f"\nNote: {total_archived} archived session(s) found{cleanup_info}.\n"
        "  These sessions were cached before their JSONL files were deleted.\n"
        "  To restore them or adjust cleanup settings, see:\n"
        "  https://github.com/daaain/claude-code-log/blob/main/dev-docs/restoring-archived-sessions.md"
    )


def process_projects_hierarchy(
    projects_path: Path,
    from_date: Optional[str] = None,
    to_date: Optional[str] = None,
    use_cache: bool = True,
    generate_individual_sessions: bool = True,
    output_format: str = "html",
    image_export_mode: Optional[str] = None,
    silent: bool = True,
    page_size: int = 2000,
    skip_combined: bool = False,
    show_stats: bool = False,
) -> Path:
    """Process the entire ~/.claude/projects/ hierarchy and create linked output files.

    Args:
        projects_path: Path to the projects directory
        from_date: Optional date filter start
        to_date: Optional date filter end
        use_cache: Whether to use SQLite cache
        generate_individual_sessions: Whether to generate per-session HTML files
        output_format: Output format (html, md, markdown)
        image_export_mode: Image export mode for markdown
        silent: If True, suppress verbose per-file logging (show summary only)
        page_size: Maximum messages per page for combined transcript pagination
        skip_combined: Skip combined transcript, generate project session index instead
    """
    import time

    start_time = time.time()

    if not projects_path.exists():
        raise FileNotFoundError(f"Projects path not found: {projects_path}")

    # Find all project directories (those with JSONL files)
    project_dirs: list[Path] = []
    for child in projects_path.iterdir():
        if child.is_dir() and list(child.glob("*.jsonl")):
            project_dirs.append(child)

    # Find archived projects (projects in cache but without JSONL files)
    archived_project_dirs: list[Path] = []
    if use_cache:
        cached_projects = get_all_cached_projects(projects_path)
        active_project_paths = {str(p) for p in project_dirs}
        for project_path_str, is_archived in cached_projects:
            if is_archived and project_path_str not in active_project_paths:
                archived_project_dirs.append(Path(project_path_str))

    if not project_dirs and not archived_project_dirs:
        raise FileNotFoundError(
            f"No project directories with JSONL files found in {projects_path}"
        )

    # Get library version for cache management
    library_version = get_library_version()

    # Process each project directory
    project_summaries: list[dict[str, Any]] = []
    any_cache_updated = False  # Track if any project had cache updates

    # Aggregated stats
    total_projects = len(project_dirs)
    projects_with_updates = 0
    total_sessions = 0
    total_archived = 0

    # Per-project stats for summary output
    project_stats: List[tuple[str, GenerationStats]] = []

    for project_dir in sorted(project_dirs):
        project_start_time = time.time()
        stats = GenerationStats()

        try:
            # Initialize cache manager for this project
            cache_manager = None
            if use_cache:
                try:
                    cache_manager = CacheManager(project_dir, library_version)
                except Exception as e:
                    stats.add_warning(f"Failed to initialize cache: {e}")

            # Phase 1: Fast check if anything needs updating (mtime comparison only)
            # Exclude agent files - they are loaded via session references, not directly
            jsonl_files = [
                f
                for f in project_dir.glob("*.jsonl")
                if not f.name.startswith("agent-")
            ]
            # Valid session IDs are from existing JSONL files (file stem = session ID)
            valid_session_ids = {f.stem for f in jsonl_files}
            modified_files = (
                cache_manager.get_modified_files(jsonl_files) if cache_manager else []
            )
            # Pass valid_session_ids to skip archived sessions (JSONL deleted)
            stale_sessions = (
                cache_manager.get_stale_sessions(
                    valid_session_ids,
                    show_stats=show_stats,
                    skip_combined=skip_combined,
                )
                if cache_manager
                else []
            )
            # Count archived sessions (cached but JSONL deleted)
            archived_count = (
                cache_manager.get_archived_session_count(valid_session_ids)
                if cache_manager
                else 0
            )
            total_archived += archived_count
            if skip_combined:
                output_path = project_dir / "index.html"
            else:
                output_path = project_dir / "combined_transcripts.html"
            # Check combined_stale using the appropriate cache:
            # - Paginated projects store data in html_pages table (via save_page_cache)
            # - Non-paginated projects store data in html_cache table (via update_html_cache)
            if skip_combined:
                if cache_manager is not None:
                    combined_stale = cache_manager.is_html_stale(
                        output_path.name,
                        None,
                        show_stats=show_stats,
                        skip_combined=skip_combined,
                    )[0]
                else:
                    combined_stale = (
                        is_html_outdated(output_path) or not output_path.exists()
                    )
            elif cache_manager is not None:
                existing_page_count = cache_manager.get_page_count()
                if existing_page_count > 0:
                    # Paginated project: check page 1 staleness
                    combined_stale = cache_manager.is_page_stale(
                        1, page_size, show_stats=show_stats
                    )[0]
                else:
                    # Non-paginated project: check html_cache
                    combined_stale = cache_manager.is_html_stale(
                        output_path.name,
                        None,
                        show_stats=show_stats,
                    )[0]
            else:
                combined_stale = True

            # Determine if we need to do any work
            needs_work = (
                bool(modified_files)
                or bool(stale_sessions)
                or combined_stale
                or not output_path.exists()
            )

            # Build archived suffix for output (shown on both cached and work paths)
            archived_suffix = (
                f", {archived_count} archived" if archived_count > 0 else ""
            )

            if not needs_work:
                # Fast path: nothing to do, just collect stats for index
                stats.files_loaded_from_cache = len(jsonl_files)
                stats.total_time = time.time() - project_start_time
                # Show progress
                print(
                    f"  {project_dir.name}: cached{archived_suffix} ({stats.total_time:.1f}s)"
                )
            else:
                # Slow path: update cache and regenerate output
                stats.files_updated = len(modified_files) if modified_files else 0
                stats.files_loaded_from_cache = len(jsonl_files) - stats.files_updated
                stats.sessions_regenerated = len(stale_sessions)

                # Track if cache was updated (for index regeneration)
                if modified_files:
                    any_cache_updated = True
                    projects_with_updates += 1
                elif combined_stale or stale_sessions:
                    any_cache_updated = True

                # Generate output for this project (handles cache updates internally)
                output_path = convert_jsonl_to(
                    output_format,
                    project_dir,
                    None,
                    from_date,
                    to_date,
                    generate_individual_sessions,
                    use_cache,
                    silent=silent,
                    image_export_mode=image_export_mode,
                    page_size=page_size,
                    skip_combined=skip_combined,
                    show_stats=show_stats,
                )

                # Track timing
                stats.total_time = time.time() - project_start_time
                # Show progress
                progress_parts: List[str] = []
                if stats.files_updated > 0:
                    progress_parts.append(f"{stats.files_updated} files updated")
                if stats.sessions_regenerated > 0:
                    progress_parts.append(f"{stats.sessions_regenerated} sessions")
                detail = ", ".join(progress_parts) if progress_parts else "regenerated"
                print(
                    f"  {project_dir.name}: {detail}{archived_suffix} ({stats.total_time:.1f}s)"
                )

            # Get project info for index - use cached data if available
            # Exclude agent files (they are loaded via session references)
            jsonl_files = [
                f
                for f in project_dir.glob("*.jsonl")
                if not f.name.startswith("agent-")
            ]
            jsonl_count = len(jsonl_files)
            last_modified: float = (
                max(f.stat().st_mtime for f in jsonl_files) if jsonl_files else 0.0
            )

            # Phase 3: Use fresh cached data for index aggregation
            if cache_manager is not None:
                cached_project_data = cache_manager.get_cached_project_data()
                if cached_project_data is not None:
                    # Track total sessions for stats
                    stats.sessions_total = len(cached_project_data.sessions)
                    # Use cached aggregation data
                    project_html_file = (
                        f"{project_dir.name}/index.html"
                        if skip_combined
                        else f"{project_dir.name}/{output_path.name}"
                    )
                    project_summaries.append(
                        {
                            "name": project_dir.name,
                            "path": project_dir,
                            "html_file": project_html_file,
                            "jsonl_count": jsonl_count,
                            "message_count": cached_project_data.total_message_count,
                            "last_modified": last_modified,
                            "total_input_tokens": cached_project_data.total_input_tokens,
                            "total_output_tokens": cached_project_data.total_output_tokens,
                            "total_cache_creation_tokens": cached_project_data.total_cache_creation_tokens,
                            "total_cache_read_tokens": cached_project_data.total_cache_read_tokens,
                            "latest_timestamp": cached_project_data.latest_timestamp,
                            "earliest_timestamp": cached_project_data.earliest_timestamp,
                            "working_directories": cache_manager.get_working_directories(),
                            "is_archived": False,
                            "sessions": [
                                {
                                    "id": session_data.session_id,
                                    "summary": session_data.custom_title
                                    or session_data.summary,
                                    "timestamp_range": format_timestamp_range(
                                        session_data.first_timestamp,
                                        session_data.last_timestamp,
                                    ),
                                    "first_timestamp": session_data.first_timestamp,
                                    "last_timestamp": session_data.last_timestamp,
                                    "message_count": session_data.message_count,
                                    "first_user_message": session_data.first_user_message
                                    or "[No user message found in session.]",
                                }
                                for session_data in cached_project_data.sessions.values()
                                # Filter out warmup-only and empty sessions (agent-only)
                                if session_data.first_user_message
                                and session_data.first_user_message != "Warmup"
                            ],
                        }
                    )
                    # Add project stats
                    project_stats.append((project_dir.name, stats))
                    continue

            # Fallback for when cache is not available (should be rare)
            print(
                f"Warning: No cached data available for {project_dir.name}, using fallback processing"
            )
            messages = load_directory_transcripts(
                project_dir, cache_manager, from_date, to_date, silent=silent
            )
            # Ensure cache is populated with session data (including working directories)
            if cache_manager:
                _update_cache_with_session_data(cache_manager, messages)
            if from_date or to_date:
                messages = filter_messages_by_date(messages, from_date, to_date)

            # Calculate token usage aggregation and find first/last interaction timestamps
            total_input_tokens = 0
            total_output_tokens = 0
            total_cache_creation_tokens = 0
            total_cache_read_tokens = 0
            latest_timestamp = ""
            earliest_timestamp = ""

            # Track requestIds to avoid double-counting tokens
            seen_request_ids: set[str] = set()

            # Collect session data for this project
            sessions_data = _collect_project_sessions(messages)

            for message in messages:
                # Track latest and earliest timestamps across all messages
                if hasattr(message, "timestamp"):
                    message_timestamp = getattr(message, "timestamp", "")
                    if message_timestamp:
                        # Track latest timestamp
                        if not latest_timestamp or message_timestamp > latest_timestamp:
                            latest_timestamp = message_timestamp

                        # Track earliest timestamp
                        if (
                            not earliest_timestamp
                            or message_timestamp < earliest_timestamp
                        ):
                            earliest_timestamp = message_timestamp

                # Calculate token usage for assistant messages
                if message.type == "assistant" and hasattr(message, "message"):
                    assistant_message = getattr(message, "message")
                    request_id = getattr(message, "requestId", None)

                    if (
                        hasattr(assistant_message, "usage")
                        and assistant_message.usage
                        and request_id
                        and request_id not in seen_request_ids
                    ):
                        # Mark requestId as seen to avoid double-counting
                        seen_request_ids.add(request_id)

                        usage = assistant_message.usage
                        total_input_tokens += usage.input_tokens or 0
                        total_output_tokens += usage.output_tokens or 0
                        if usage.cache_creation_input_tokens:
                            total_cache_creation_tokens += (
                                usage.cache_creation_input_tokens
                            )
                        if usage.cache_read_input_tokens:
                            total_cache_read_tokens += usage.cache_read_input_tokens

            project_summaries.append(
                {
                    "name": project_dir.name,
                    "path": project_dir,
                    "html_file": f"{project_dir.name}/{output_path.name}",
                    "jsonl_count": jsonl_count,
                    "message_count": len(messages),
                    "last_modified": last_modified,
                    "total_input_tokens": total_input_tokens,
                    "total_output_tokens": total_output_tokens,
                    "total_cache_creation_tokens": total_cache_creation_tokens,
                    "total_cache_read_tokens": total_cache_read_tokens,
                    "latest_timestamp": latest_timestamp,
                    "earliest_timestamp": earliest_timestamp,
                    "working_directories": cache_manager.get_working_directories()
                    if cache_manager
                    else [],
                    "is_archived": False,
                    "sessions": sessions_data,
                }
            )
            # Track session count in stats for fallback path
            stats.sessions_total = len(sessions_data)
            project_stats.append((project_dir.name, stats))

        except Exception as e:
            prev_project = project_summaries[-1] if project_summaries else "(none)"
            stats.add_error(str(e))
            project_stats.append((project_dir.name, stats))
            print(
                f"Warning: Failed to process {project_dir}: {e}\n"
                f"Previous (in alphabetical order) project before error: {prev_project}"
                f"\n{traceback.format_exc()}"
            )
            continue

    # Process archived projects (projects in cache but without JSONL files)
    archived_project_count = 0
    for archived_dir in sorted(archived_project_dirs):
        try:
            # Initialize cache manager for archived project
            cache_manager = CacheManager(archived_dir, library_version)
            cached_project_data = cache_manager.get_cached_project_data()

            if cached_project_data is None:
                continue

            archived_project_count += 1
            print(
                f"  {archived_dir.name}: [ARCHIVED] ({len(cached_project_data.sessions)} sessions)"
            )

            # Add archived project to summaries
            project_summaries.append(
                {
                    "name": archived_dir.name,
                    "path": archived_dir,
                    "html_file": f"{archived_dir.name}/index.html"
                    if skip_combined
                    else f"{archived_dir.name}/combined_transcripts.html",
                    "jsonl_count": 0,
                    "message_count": cached_project_data.total_message_count,
                    "last_modified": 0.0,
                    "total_input_tokens": cached_project_data.total_input_tokens,
                    "total_output_tokens": cached_project_data.total_output_tokens,
                    "total_cache_creation_tokens": cached_project_data.total_cache_creation_tokens,
                    "total_cache_read_tokens": cached_project_data.total_cache_read_tokens,
                    "latest_timestamp": cached_project_data.latest_timestamp,
                    "earliest_timestamp": cached_project_data.earliest_timestamp,
                    "working_directories": cache_manager.get_working_directories(),
                    "is_archived": True,
                    "sessions": [
                        {
                            "id": session_data.session_id,
                            "summary": session_data.custom_title
                            or session_data.summary,
                            "timestamp_range": format_timestamp_range(
                                session_data.first_timestamp,
                                session_data.last_timestamp,
                            ),
                            "first_timestamp": session_data.first_timestamp,
                            "last_timestamp": session_data.last_timestamp,
                            "message_count": session_data.message_count,
                            "first_user_message": session_data.first_user_message
                            or "[No user message found in session.]",
                        }
                        for session_data in cached_project_data.sessions.values()
                        if session_data.first_user_message
                        and session_data.first_user_message != "Warmup"
                    ],
                }
            )
        except Exception as e:
            print(f"Warning: Failed to process archived project {archived_dir}: {e}")
            continue

    # Update total projects count to include archived
    total_projects = len(project_dirs) + archived_project_count

    # Generate index (always regenerate if outdated)
    ext = get_file_extension(output_format)
    index_path = projects_path / f"index.{ext}"
    renderer = get_renderer(output_format, image_export_mode)
    index_regenerated = False
    if renderer.is_outdated(index_path) or from_date or to_date or any_cache_updated:
        index_content = renderer.generate_projects_index(
            project_summaries, from_date, to_date, show_stats=show_stats
        )
        assert index_content is not None
        index_path.write_text(index_content, encoding="utf-8")
        index_regenerated = True
    elif not silent:
        print(f"Index {ext.upper()} is current, skipping regeneration")

    # Count total sessions from project summaries
    for summary in project_summaries:
        total_sessions += len(summary.get("sessions", []))

    # Print summary
    elapsed = time.time() - start_time

    # Print any errors/warnings that occurred
    for project_name, stats in project_stats:
        for warning in stats.warnings:
            print(f"  Warning ({project_name}): {warning}")
        for error in stats.errors:
            print(f"  Error ({project_name}): {error}")

    # Global summary
    summary_parts: List[str] = []
    summary_parts.append(f"Processed {total_projects} projects in {elapsed:.1f}s")
    if projects_with_updates > 0:
        summary_parts.append(f"  {projects_with_updates} projects updated")
    if index_regenerated:
        summary_parts.append("  Index regenerated")
    print("\n".join(summary_parts))

    # Show archived sessions note if any exist
    if total_archived > 0:
        _print_archived_sessions_note(total_archived)

    return index_path
